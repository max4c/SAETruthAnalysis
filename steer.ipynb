{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # for google colab users\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "  # for local setup\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"15\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not(COLAB):\n",
    "        webbrowser.open(filename);\n",
    "\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
    "\n",
    "        PORT += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# package import\n",
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model mistral-7b into HookedTransformer\n",
      "blocks.16.hook_resid_pre\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "# Choose a layer you want to focus on\n",
    "# For this tutorial, we're going to use layer 2\n",
    "layer = 16\n",
    "\n",
    "# get model\n",
    "model = HookedTransformer.from_pretrained(\"mistral-7b\", device = device)\n",
    "\n",
    "# get the SAE for this layer\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release=\"mistral-7b-res-wg\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=f\"blocks.{layer}.hook_resid_pre\",  # won't always be a hook point\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name\n",
    "print(hook_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 28705,   415, 14739, 19986, 15050]], device='cuda:0')\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[6.0901e+01, 2.6774e+01, 5.6409e-03],\n",
      "         [4.0106e+01, 2.8247e+01, 1.2325e+01],\n",
      "         [1.7201e+01, 1.3086e+01, 1.0442e+01],\n",
      "         [1.9446e+01, 1.3755e+01, 8.6492e+00],\n",
      "         [1.1657e+01, 9.6264e+00, 7.7555e+00],\n",
      "         [1.4528e+01, 1.1941e+01, 1.1077e+01]]], device='cuda:0'),\n",
      "indices=tensor([[[ 9725, 59488, 53699],\n",
      "         [10855, 61479,  9725],\n",
      "         [45837, 51897, 53690],\n",
      "         [54117,  2786, 61771],\n",
      "         [53015, 49200, 39893],\n",
      "         [17640, 20150, 39893]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sv_prompt = \" The Golden Gate Bridge\"\n",
    "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(sv_prompt)\n",
    "print(tokens)\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts = sae.encode(cache[hook_point])\n",
    "\n",
    "# get sae_out\n",
    "sae_out = sae.decode(sv_feature_acts)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "print(torch.topk(sv_feature_acts, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector = sae.W_dec[12590]\n",
    "\n",
    "example_prompt = \"Who actually said Let them eat cake?\"\n",
    "coeff = -300\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "      # using our steering vector and applying the coefficient\n",
    "      resid_pre[:, :position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            **kwargs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "  model.reset_hooks()\n",
    "  editing_hooks = [(f\"blocks.{layer}.hook_resid_post\", steering_hook)]\n",
    "  res = hooked_generate([example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs)\n",
    "\n",
    "  # Print results, removing the ugly beginning of sequence token\n",
    "  res_str = model.to_string(res[:, 1:])\n",
    "  print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who actually said Let them eat cake?\n",
      "\n",
      "The phrase “Let them eat cake” is often attributed to Marie Antoinette, the wife of King Louis XVI of France. The story goes that when she was told that the peasants had no bread to eat, she responded with this\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Who actually said Let them eat cake?\n",
      "\n",
      "Marie Antoinette is often credited with the quote, “Let them eat cake,” but it’s not true. The phrase was first used by Jean-Jacques Rousseau in his 1755 novel\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Who actually said Let them eat cake?\n",
      "\n",
      "The phrase “Let them eat cake” is often attributed to Marie Antoinette, the wife of King Louis XVI of France. The story goes that when she was told that the peasants had no bread to eat, she responded with this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steering_on = True\n",
    "run_generate(example_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
